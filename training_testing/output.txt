üöÄ ÂêØÂä®4Âç°ËÆ≠ÁªÉ...
ÈÖçÁΩÆ:
  - GPUs: 4
  - Batch size per GPU: 1
  - Max sequence length: 8192
  - Gradient accumulation: 4 (effective batch size = 4*1*4 = 16)
  - LoRA rank: 32

INFO:__main__:Global rank: 0, Local rank: 0
INFO:__main__:Loading model and tokenizer from OpenGVLab/InternVL3-8B
INFO:__main__:Global rank: 2, Local rank: 2
INFO:__main__:Loading model and tokenizer from OpenGVLab/InternVL3-8B
INFO:__main__:Global rank: 1, Local rank: 1
INFO:__main__:Loading model and tokenizer from OpenGVLab/InternVL3-8B
INFO:__main__:Global rank: 3, Local rank: 3
INFO:__main__:Loading model and tokenizer from OpenGVLab/InternVL3-8B
loading file vocab.json from cache at /scratch/czr/huggingface/hub/models--OpenGVLab--InternVL3-8B/snapshots/24dc81a234a6e1901f3314eeadaa2813f2b78038/vocab.json
loading file merges.txt from cache at /scratch/czr/huggingface/hub/models--OpenGVLab--InternVL3-8B/snapshots/24dc81a234a6e1901f3314eeadaa2813f2b78038/merges.txt
loading file added_tokens.json from cache at /scratch/czr/huggingface/hub/models--OpenGVLab--InternVL3-8B/snapshots/24dc81a234a6e1901f3314eeadaa2813f2b78038/added_tokens.json
loading file special_tokens_map.json from cache at /scratch/czr/huggingface/hub/models--OpenGVLab--InternVL3-8B/snapshots/24dc81a234a6e1901f3314eeadaa2813f2b78038/special_tokens_map.json
loading file tokenizer_config.json from cache at /scratch/czr/huggingface/hub/models--OpenGVLab--InternVL3-8B/snapshots/24dc81a234a6e1901f3314eeadaa2813f2b78038/tokenizer_config.json
loading file tokenizer.json from cache at /scratch/czr/huggingface/hub/models--OpenGVLab--InternVL3-8B/snapshots/24dc81a234a6e1901f3314eeadaa2813f2b78038/tokenizer.json
loading file chat_template.jinja from cache at None
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
loading configuration file config.json from cache at /scratch/czr/huggingface/hub/models--OpenGVLab--InternVL3-8B/snapshots/24dc81a234a6e1901f3314eeadaa2813f2b78038/config.json
loading configuration file config.json from cache at /scratch/czr/huggingface/hub/models--OpenGVLab--InternVL3-8B/snapshots/24dc81a234a6e1901f3314eeadaa2813f2b78038/config.json
INFO:transformers_modules.OpenGVLab.InternVL3-8B.24dc81a234a6e1901f3314eeadaa2813f2b78038.configuration_internvl_chat:vision_select_layer: -1
INFO:transformers_modules.OpenGVLab.InternVL3-8B.24dc81a234a6e1901f3314eeadaa2813f2b78038.configuration_internvl_chat:ps_version: v2
INFO:transformers_modules.OpenGVLab.InternVL3-8B.24dc81a234a6e1901f3314eeadaa2813f2b78038.configuration_internvl_chat:min_dynamic_patch: 1
INFO:transformers_modules.OpenGVLab.InternVL3-8B.24dc81a234a6e1901f3314eeadaa2813f2b78038.configuration_internvl_chat:max_dynamic_patch: 12
INFO:transformers_modules.OpenGVLab.InternVL3-8B.24dc81a234a6e1901f3314eeadaa2813f2b78038.configuration_internvl_chat:vision_config is None. Initializing the InternVisionConfig with default values.
INFO:transformers_modules.OpenGVLab.InternVL3-8B.24dc81a234a6e1901f3314eeadaa2813f2b78038.configuration_internvl_chat:llm_config is None. Initializing the LlamaConfig config with default values (`LlamaConfig`).
INFO:transformers_modules.OpenGVLab.InternVL3-8B.24dc81a234a6e1901f3314eeadaa2813f2b78038.configuration_internvl_chat:vision_select_layer: -1
INFO:transformers_modules.OpenGVLab.InternVL3-8B.24dc81a234a6e1901f3314eeadaa2813f2b78038.configuration_internvl_chat:ps_version: v1
INFO:transformers_modules.OpenGVLab.InternVL3-8B.24dc81a234a6e1901f3314eeadaa2813f2b78038.configuration_internvl_chat:min_dynamic_patch: 1
INFO:transformers_modules.OpenGVLab.InternVL3-8B.24dc81a234a6e1901f3314eeadaa2813f2b78038.configuration_internvl_chat:max_dynamic_patch: 6
Model config InternVLChatConfig {
  "architectures": [
    "InternVLChatModel"
  ],
  "auto_map": {
    "AutoConfig": "configuration_internvl_chat.InternVLChatConfig",
    "AutoModel": "modeling_internvl_chat.InternVLChatModel",
    "AutoModelForCausalLM": "modeling_internvl_chat.InternVLChatModel"
  },
  "downsample_ratio": 0.5,
  "dynamic_image_size": true,
  "force_image_size": 448,
  "hidden_size": 3584,
  "image_fold": null,
  "llm_config": {
    "_attn_implementation_autoset": true,
    "_name_or_path": "./pretrained/Qwen2.5-32B-Instruct",
    "architectures": [
      "Qwen2ForCausalLM"
    ],
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "eos_token_id": 151643,
    "hidden_act": "silu",
    "hidden_size": 3584,
    "initializer_range": 0.02,
    "intermediate_size": 18944,
    "layer_types": [
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention"
    ],
    "max_position_embeddings": 32768,
    "max_window_layers": 70,
    "model_type": "qwen2",
    "moe_config": null,
    "num_attention_heads": 28,
    "num_hidden_layers": 28,
    "num_key_value_heads": 4,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "factor": 2.0,
      "rope_type": "dynamic",
      "type": "dynamic"
    },
    "rope_theta": 1000000.0,
    "sliding_window": null,
    "torch_dtype": "bfloat16",
    "use_bfloat16": true,
    "use_cache": false,
    "use_sliding_window": false,
    "vocab_size": 151674
  },
  "max_dynamic_patch": 12,
  "min_dynamic_patch": 1,
  "model_type": "internvl_chat",
  "output_attentions": false,
  "pad2square": false,
  "ps_version": "v2",
  "select_layer": -1,
  "system_message": null,
  "template": "internvl2_5",
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": null,
  "use_backbone_lora": 0,
  "use_llm_lora": 0,
  "use_thumbnail": true,
  "vision_config": {
    "_attn_implementation_autoset": true,
    "_name_or_path": "OpenGVLab/InternViT-6B-448px-V1-5",
    "architectures": [
      "InternVisionModel"
    ],
    "attention_dropout": 0.0,
    "auto_map": {
      "AutoConfig": "configuration_intern_vit.InternVisionConfig",
      "AutoModel": "modeling_intern_vit.InternVisionModel"
    },
    "capacity_factor": 1.2,
    "drop_path_rate": 0.1,
    "dropout": 0.0,
    "eval_capacity_factor": 1.4,
    "hidden_act": "gelu",
    "hidden_size": 1024,
    "image_size": 448,
    "initializer_factor": 0.1,
    "initializer_range": 1e-10,
    "intermediate_size": 4096,
    "laux_allreduce": "all_nodes",
    "layer_norm_eps": 1e-06,
    "model_type": "intern_vit_6b",
    "moe_coeff_ratio": 0.5,
    "moe_intermediate_size": 768,
    "moe_output_scale": 4.0,
    "noisy_gate_policy": "RSample_before",
    "norm_type": "layer_norm",
    "num_attention_heads": 16,
    "num_channels": 3,
    "num_experts": 8,
    "num_hidden_layers": 24,
    "num_routed_experts": 4,
    "num_shared_experts": 4,
    "patch_size": 14,
    "qk_normalization": false,
    "qkv_bias": true,
    "shared_expert_intermediate_size": 3072,
    "torch_dtype": "bfloat16",
    "use_bfloat16": true,
    "use_flash_attn": true,
    "use_moe": false,
    "use_residual": true,
    "use_rts": false,
    "use_weighted_residual": false
  }
}

INFO:transformers_modules.OpenGVLab.InternVL3-8B.24dc81a234a6e1901f3314eeadaa2813f2b78038.configuration_internvl_chat:vision_select_layer: -1
INFO:transformers_modules.OpenGVLab.InternVL3-8B.24dc81a234a6e1901f3314eeadaa2813f2b78038.configuration_internvl_chat:ps_version: v2
INFO:transformers_modules.OpenGVLab.InternVL3-8B.24dc81a234a6e1901f3314eeadaa2813f2b78038.configuration_internvl_chat:min_dynamic_patch: 1
INFO:transformers_modules.OpenGVLab.InternVL3-8B.24dc81a234a6e1901f3314eeadaa2813f2b78038.configuration_internvl_chat:max_dynamic_patch: 12
INFO:transformers_modules.OpenGVLab.InternVL3-8B.24dc81a234a6e1901f3314eeadaa2813f2b78038.configuration_internvl_chat:vision_config is None. Initializing the InternVisionConfig with default values.
INFO:transformers_modules.OpenGVLab.InternVL3-8B.24dc81a234a6e1901f3314eeadaa2813f2b78038.configuration_internvl_chat:llm_config is None. Initializing the LlamaConfig config with default values (`LlamaConfig`).
INFO:transformers_modules.OpenGVLab.InternVL3-8B.24dc81a234a6e1901f3314eeadaa2813f2b78038.configuration_internvl_chat:vision_select_layer: -1
INFO:transformers_modules.OpenGVLab.InternVL3-8B.24dc81a234a6e1901f3314eeadaa2813f2b78038.configuration_internvl_chat:ps_version: v1
INFO:transformers_modules.OpenGVLab.InternVL3-8B.24dc81a234a6e1901f3314eeadaa2813f2b78038.configuration_internvl_chat:min_dynamic_patch: 1
INFO:transformers_modules.OpenGVLab.InternVL3-8B.24dc81a234a6e1901f3314eeadaa2813f2b78038.configuration_internvl_chat:max_dynamic_patch: 6
INFO:transformers_modules.OpenGVLab.InternVL3-8B.24dc81a234a6e1901f3314eeadaa2813f2b78038.configuration_internvl_chat:vision_select_layer: -1
INFO:transformers_modules.OpenGVLab.InternVL3-8B.24dc81a234a6e1901f3314eeadaa2813f2b78038.configuration_internvl_chat:ps_version: v2
INFO:transformers_modules.OpenGVLab.InternVL3-8B.24dc81a234a6e1901f3314eeadaa2813f2b78038.configuration_internvl_chat:min_dynamic_patch: 1
INFO:transformers_modules.OpenGVLab.InternVL3-8B.24dc81a234a6e1901f3314eeadaa2813f2b78038.configuration_internvl_chat:max_dynamic_patch: 12
INFO:transformers_modules.OpenGVLab.InternVL3-8B.24dc81a234a6e1901f3314eeadaa2813f2b78038.configuration_internvl_chat:vision_config is None. Initializing the InternVisionConfig with default values.
INFO:transformers_modules.OpenGVLab.InternVL3-8B.24dc81a234a6e1901f3314eeadaa2813f2b78038.configuration_internvl_chat:llm_config is None. Initializing the LlamaConfig config with default values (`LlamaConfig`).
INFO:transformers_modules.OpenGVLab.InternVL3-8B.24dc81a234a6e1901f3314eeadaa2813f2b78038.configuration_internvl_chat:vision_select_layer: -1
INFO:transformers_modules.OpenGVLab.InternVL3-8B.24dc81a234a6e1901f3314eeadaa2813f2b78038.configuration_internvl_chat:ps_version: v1
INFO:transformers_modules.OpenGVLab.InternVL3-8B.24dc81a234a6e1901f3314eeadaa2813f2b78038.configuration_internvl_chat:min_dynamic_patch: 1
INFO:transformers_modules.OpenGVLab.InternVL3-8B.24dc81a234a6e1901f3314eeadaa2813f2b78038.configuration_internvl_chat:max_dynamic_patch: 6
INFO:transformers_modules.OpenGVLab.InternVL3-8B.24dc81a234a6e1901f3314eeadaa2813f2b78038.configuration_internvl_chat:vision_select_layer: -1
INFO:transformers_modules.OpenGVLab.InternVL3-8B.24dc81a234a6e1901f3314eeadaa2813f2b78038.configuration_internvl_chat:ps_version: v2
INFO:transformers_modules.OpenGVLab.InternVL3-8B.24dc81a234a6e1901f3314eeadaa2813f2b78038.configuration_internvl_chat:min_dynamic_patch: 1
INFO:transformers_modules.OpenGVLab.InternVL3-8B.24dc81a234a6e1901f3314eeadaa2813f2b78038.configuration_internvl_chat:max_dynamic_patch: 12
INFO:transformers_modules.OpenGVLab.InternVL3-8B.24dc81a234a6e1901f3314eeadaa2813f2b78038.configuration_internvl_chat:vision_config is None. Initializing the InternVisionConfig with default values.
INFO:transformers_modules.OpenGVLab.InternVL3-8B.24dc81a234a6e1901f3314eeadaa2813f2b78038.configuration_internvl_chat:llm_config is None. Initializing the LlamaConfig config with default values (`LlamaConfig`).
INFO:transformers_modules.OpenGVLab.InternVL3-8B.24dc81a234a6e1901f3314eeadaa2813f2b78038.configuration_internvl_chat:vision_select_layer: -1
INFO:transformers_modules.OpenGVLab.InternVL3-8B.24dc81a234a6e1901f3314eeadaa2813f2b78038.configuration_internvl_chat:ps_version: v1
INFO:transformers_modules.OpenGVLab.InternVL3-8B.24dc81a234a6e1901f3314eeadaa2813f2b78038.configuration_internvl_chat:min_dynamic_patch: 1
INFO:transformers_modules.OpenGVLab.InternVL3-8B.24dc81a234a6e1901f3314eeadaa2813f2b78038.configuration_internvl_chat:max_dynamic_patch: 6
loading weights file model.safetensors from cache at /scratch/czr/huggingface/hub/models--OpenGVLab--InternVL3-8B/snapshots/24dc81a234a6e1901f3314eeadaa2813f2b78038/model.safetensors.index.json
Instantiating InternVLChatModel model under default dtype torch.bfloat16.
Generate config GenerationConfig {}

INFO:transformers_modules.OpenGVLab.InternVL3-8B.24dc81a234a6e1901f3314eeadaa2813f2b78038.modeling_internvl_chat:num_image_token: 256
INFO:transformers_modules.OpenGVLab.InternVL3-8B.24dc81a234a6e1901f3314eeadaa2813f2b78038.modeling_internvl_chat:ps_version: v2
INFO:transformers_modules.OpenGVLab.InternVL3-8B.24dc81a234a6e1901f3314eeadaa2813f2b78038.modeling_internvl_chat:num_image_token: 256
INFO:transformers_modules.OpenGVLab.InternVL3-8B.24dc81a234a6e1901f3314eeadaa2813f2b78038.modeling_internvl_chat:ps_version: v2
INFO:transformers_modules.OpenGVLab.InternVL3-8B.24dc81a234a6e1901f3314eeadaa2813f2b78038.modeling_internvl_chat:num_image_token: 256
INFO:transformers_modules.OpenGVLab.InternVL3-8B.24dc81a234a6e1901f3314eeadaa2813f2b78038.modeling_internvl_chat:ps_version: v2
INFO:transformers_modules.OpenGVLab.InternVL3-8B.24dc81a234a6e1901f3314eeadaa2813f2b78038.modeling_internvl_chat:num_image_token: 256
INFO:transformers_modules.OpenGVLab.InternVL3-8B.24dc81a234a6e1901f3314eeadaa2813f2b78038.modeling_internvl_chat:ps_version: v2
Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "use_cache": false
}

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:00<00:00, 28.82it/s]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 38.11it/s]
All model checkpoint weights were used when initializing InternVLChatModel.

All the weights of InternVLChatModel were initialized from the model checkpoint at OpenGVLab/InternVL3-8B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use InternVLChatModel for predictions without further training.
Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:00<00:00, 28.91it/s]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 38.22it/s]
Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:00<00:00, 28.75it/s]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 38.01it/s]
Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:00<00:00, 29.08it/s]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 38.44it/s]
loading configuration file generation_config.json from cache at /scratch/czr/huggingface/hub/models--OpenGVLab--InternVL3-8B/snapshots/24dc81a234a6e1901f3314eeadaa2813f2b78038/generation_config.json
Generate config GenerationConfig {}

INFO:__main__:img_context_token_id: 151667
INFO:__main__:Original max_dynamic_patch: 12
INFO:__main__:Updated max_dynamic_patch: 12
INFO:__main__:Applying LoRA configuration...

‚úÖ ËÆ≠ÁªÉÂÆåÊàêÔºÅ
Ê®°Âûã‰øùÂ≠òÂú®: ./output_4gpu_bs2_8k
ËÆ≠ÁªÉÊó•Âøó: ./output_4gpu_bs2_8k/training.log
